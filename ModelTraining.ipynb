{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a83ca55",
   "metadata": {},
   "source": [
    "## Milestone 1: Problem Definition, Dataset Selection, and Data Exploration\n",
    "\n",
    "LIS 640 - Introduction to Applied Deep Learning\n",
    "\n",
    "Due 2/14/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da321fe4",
   "metadata": {},
   "source": [
    "## **Overview**\n",
    "In this milestone, you will:\n",
    "1. **Define a deep learning problem** where AI can make a meaningful impact.\n",
    "2. **Identify three datasets** that fit your topic and justify their relevance.\n",
    "3. **Explore and visualize** the datasets to understand their structure.\n",
    "4. **Implement a PyTorch Dataset class** to prepare data for deep learning.\n",
    "\n",
    "This notebook provides an example of **fuel-efficient car usage** to illustrate what is expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba4078e",
   "metadata": {},
   "source": [
    "## **Step 1: Define Your Deep Learning Problem**\n",
    "Write a paragraph explaining:\n",
    "- **Why your chosen topic is important.**\n",
    "- **How deep learning can help solve the problem.**\n",
    "\n",
    "### **Example Problem Statement: Fuel-Efficient Car Usage**\n",
    "*Fuel efficiency is a major factor in reducing carbon emissions and lowering fuel costs. Drivers often adopt inefficient driving patterns, wasting fuel through unnecessary acceleration, braking, or idling. A deep learning model could analyze driving behavior and suggest optimizations in real-time, helping individuals improve their fuel economy.*\n",
    "\n",
    "âž¡ **Write your problem statement below:**  \n",
    "\n",
    "### **Lane Line Detection**\n",
    "Lane line detection is a crucial aspect of most autonomous driving systems. It involves identifying and tracking the lane markings on the road to ensure the vehicle stays within its designated lane. Accurate lane detection is essential for maintaining safe driving conditions, enabling features like lane-keeping assistance, adaptive cruise control, and autonomous navigation. However, challenges such as varying lighting conditions, occlusions (e.g., by other vehicles or debris), and poorly marked or faded lane lines can make this task complex. A deep learning model trained on annotated road images can be used to detect lane lines in real-time, providing the vehicle with the necessary information to make informed decisions and navigate safely. By improving the robustness and accuracy of lane detection systems, we can enhance the safety and reliability of autonomous vehicles, ultimately contributing to safer roads and more efficient transportation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb8cc8",
   "metadata": {},
   "source": [
    "## **Step 2: Identify and Justify Three Relevant Datasets**\n",
    "Find three datasets that provide useful information for solving your problem.  \n",
    "For each dataset, include:\n",
    "1. A **short description** of what it contains.\n",
    "2. A **link to the dataset**.\n",
    "3. **Why this dataset is useful for your problem.**\n",
    "\n",
    "### **Example Datasets for Fuel Efficiency**\n",
    "\n",
    "- **Dataset 1: Vehicle Trajectory Data (NGSIM US 101 Dataset)**\n",
    "\t- Description: This dataset contains detailed vehicle trajectory data collected on a segment of the U.S. Highway 101 in Los Angeles, California. It includes precise location information of each vehicle within the study area every one-tenth of a second, capturing detailed lane-changing and car-following behaviors.\n",
    "\t- Source: [U.S. Department of Transportation - NGSIM Program](https://data.transportation.gov/stories/s/Next-Generation-Simulation-NGSIM-Open-Data/i5zb-xe34/)\n",
    "\t- Justification: Analyzing this data can help identify driving patterns that affect fuel efficiency, such as frequent lane changes or abrupt braking.\n",
    "\n",
    "- **Dataset 2: Climate & Air Quality Data**  \n",
    "  - Description: Contains CO2 emissions and climate-related metrics across different regions.\n",
    "\tâ€¢\tSource: [U.S. Historical Climatology Network](https://www.ncei.noaa.gov/products/land-based-station/us-historical-climatology-network)\n",
    "\tâ€¢\tJustification: Can correlate driving behavior with environmental impact. Provides environmental context to fuel consumption.\n",
    "\n",
    "- **Dataset 3: Automobile Dataset (UCI Machine Learning Repository)**\n",
    "  - Description: This dataset includes various characteristics of automobiles, such as engine size, horsepower, weight, and fuel consumption. It also provides information on insurance risk ratings and normalized losses in use as compared to other cars.\n",
    "  - Source: [UCI Machine Learning Repository - Automobile Dataset](https://archive.ics.uci.edu/dataset/10/automobile)\n",
    "  - Justification: The datasetâ€™s detailed vehicle specifications and performance metrics can be used to analyze how different factors influence fuel efficiency, aiding in the development of predictive models.\n",
    "\n",
    "âž¡ **Find and document three datasets for your problem below:**\n",
    "\n",
    "- **Dataset 1: Lane Line detection Dataset**\n",
    "\t- Description: This dataset has 100 images from German roads including annotated lane lines in each image. The dataset is diverse including curved roads.\n",
    "\t- Source: [New-Lane detection Computer Vision](https://universe.roboflow.com/maanasa-prasad/new-lane-detection)\n",
    "\t- Justification: Provides lane line data from different environments and includes curved lane lines.\n",
    "\n",
    "- **Dataset 2: Indian Roads Dataset**\n",
    "\t- Description: This dataset contains over 650 labeled lane images of various road environments, such as curves, traffic, and more. It is collected from real scenarios across multiple cities in India, and includes images with lane lines that are manually annotated with polygons.\n",
    "\t- Source: [Lane Detection Computer Vision Project](https://universe.roboflow.com/autonomous-umjvo/lane-detection-2-qpx6p)\n",
    "\t- Justification: Provides lane line data from a different road driving setting, allowing us to get a better variety of data (i.e. left/right hand drive, different road markings, etc).\n",
    "\n",
    "- **Dataset 3: US Roads Dataset**\n",
    "\t- Description: This dataset contains over 100 labeled lane images from highway driving in the United States in clear conditions. It includes images with the relevant lanes and lane lines annotated with polygons.\n",
    "\t- Source: [Lane Detection Computer Vision Group](https://universe.roboflow.com/computer-vision-controls-research-group/lane-detection-tjaa0)\n",
    "\t- Justification: Provides lane line data from a more local and potentially more relevant setting where lane line detection might be more necessary â€“ ADAS/hands-off cruise control and steering for long highway drives.\n",
    "\n",
    "\n",
    "\n",
    "### We also found these datasets, but have not analyzed them yet due to the size and might explore them later if possible\n",
    "- **Dataset 1: CurveLanes Dataset from Kaggle**\n",
    "\t- Description: This dataset has 150k lane images of difficult scenarios such as curves and multi-lanes in traffic. It is collected from real urban and highway scenarios in multiple cities in China. The dataset includes images with lane lines which are manually annotated with natural cubic splines. The labels include two key x, y coordinates of the lane marking.\n",
    "\t- Source: [Kaggle CurveLanes Dataset](https://www.kaggle.com/datasets/bnyadmohammed/curvelanes/data) and uploaded from [Github CurveLanes Dataset](https://github.com/SoulmateB/CurveLanes)\n",
    "\t- Justification: The dataset includes more difficult to detect lane lines in more complex and variety of scenarios\n",
    "\n",
    "- **Dataset 2: Waymo Open Dataset - Motion Dataset**\n",
    "\t- Description: This dataset has lane line data which was used internally by Waymo for their training purposes which has been open sourced. It includes lane connections, lane boundaries and lane neighbors. It provies information of multiple x, y coordinates along the lane line as labels.\n",
    "\t- Source [Waymo Open Dataset](https://github.com/waymo-research/waymo-open-dataset?tab=readme-ov-file)\n",
    "\t- Justification: This data includes detailed information on more lane line data but also includes features such as lane neighbors and lane connections.\n",
    "\n",
    "- **Dataset 3: TuSimple Lane Line Dataset**\n",
    "\t- Description: The dataset consists of 6,408 road images on US highways and includes images from different weather conditions. Dataset includes annotated lane lines.\n",
    "\t- Source [TuSimple Dataset on Kaggle](https://www.kaggle.com/datasets/manideep1108/tusimple)\n",
    "\t- Justification: This dataset emphasizes variation in weather conditions which the other datasets do not mention which will allow our model to generalize better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e90508",
   "metadata": {},
   "source": [
    "## **Step 3: Explore and Visualize Your Data**\n",
    "Understanding the structure of your dataset is crucial. Perform the following tasks:\n",
    "1. **Summarize dataset statistics:**\n",
    "   - Number of samples\n",
    "   - Number of features\n",
    "   - Data types (numerical, categorical, text, etc.)\n",
    "   - Ranges of values (min/max)\n",
    "   - Missing values\n",
    "\n",
    "2. **Create visualizations:**\n",
    "   - Histograms: Show feature distributions.\n",
    "   - Scatter plots: Explore relationships between key variables.\n",
    "   - (Optional) PCA: Visualize high-dimensional data in 2D.\n",
    "\n",
    "### **Example Exploration Code**\n",
    "Modify this code to work with your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "datasets = {\n",
    "    \"German\": {\n",
    "        \"image_dir\": \"../Maanasa/train/images\",\n",
    "        \"label_dir\": \"../Maanasa/train/labels\"\n",
    "    },\n",
    "    \"Indian\": {\n",
    "        \"image_dir\": \"../Autonomous/train/images\",\n",
    "        \"label_dir\": \"../Autonomous/train/labels\"\n",
    "    },\n",
    "    \"US\": {\n",
    "        \"image_dir\": \"../ComputerVisionGroup/train/images\",\n",
    "        \"label_dir\": \"../ComputerVisionGroup/train/labels\"\n",
    "    }\n",
    "}\n",
    "\n",
    "total_images = 0\n",
    "total_labels = 0\n",
    "all_dims = []\n",
    "\n",
    "for name, paths in datasets.items():\n",
    "    image_dir = paths[\"image_dir\"]\n",
    "    label_dir = paths[\"label_dir\"]\n",
    "\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]\n",
    "\n",
    "    total_images += len(image_files)\n",
    "    total_labels += len(label_files)\n",
    "\n",
    "    print(f\"{name} Dataset:\")\n",
    "    print(f\"  Total images: {len(image_files)}\")\n",
    "    print(f\"  Total label files: {len(label_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4829460c",
   "metadata": {},
   "source": [
    "This gives us an understanding of how much data we have for each dataset. If the datasets have certain trends, we see that the Indian roads dataset is the primary source of data, meaning that our model may be more heavily suited for inference in those conditions. This suggests that we may need to find more data, such as the large datasets we listed above (Waymo, TuSimple, Kaggle curvelines), and incorperate those into our combined dataset in addition to these, to get more accuracy for all different conditions the vehicle may be in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75704179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lane_points(ax, image_path, label_path):\n",
    "    \"\"\"Plot lane lines on an image using the given axes.\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    img = np.array(img)\n",
    "\n",
    "    lanes = []\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = list(map(float, line.strip().split()))\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            points = np.array(parts[1:]).reshape(-1, 2)\n",
    "            lanes.append(points)\n",
    "\n",
    "    ax.imshow(img)\n",
    "    for lane in lanes:\n",
    "        ax.plot(lane[:, 0] * img.shape[1], lane[:, 1] * img.shape[0], \n",
    "                 marker='o', linestyle='-', markersize=4, label=\"Lane\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 5))\n",
    "\n",
    "for i, (name, dataset) in enumerate(datasets.items()):\n",
    "    image_dir = dataset[\"image_dir\"]\n",
    "    label_dir = dataset[\"label_dir\"]\n",
    "    image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "    random_file = random.choice(image_files)\n",
    "    label_file = random_file.replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "    plot_lane_points(axes[i], os.path.join(image_dir, random_file), os.path.join(label_dir, label_file))\n",
    "    axes[i].set_title(name)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf4275",
   "metadata": {},
   "source": [
    "This visualization gives us a visual understanding of the labels for all of our datasets. We see that the Indian roads data set seems to use a different format for its labels, as it uses one big polygon for the area inside the lane lines, while the other two use polygons for each lane line and lane line segment. This may cause issues when we train our model, and we may need to reconfigure the Indian roads labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecaca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lane_counts = []\n",
    "\n",
    "for name, paths in datasets.items():\n",
    "    label_dir = paths[\"label_dir\"]\n",
    "    label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]\n",
    "\n",
    "    lane_counts = []\n",
    "    for label_file in label_files:\n",
    "        with open(os.path.join(label_dir, label_file), \"r\") as f:\n",
    "            lane_counts.append(len(f.readlines()))\n",
    "\n",
    "    all_lane_counts.extend(lane_counts)\n",
    "\n",
    "min_value = min(all_lane_counts)\n",
    "max_value = max(all_lane_counts)\n",
    "\n",
    "bins = np.arange(min_value, max_value + 2) - 0.5\n",
    "\n",
    "plt.hist(all_lane_counts, bins=bins, edgecolor=\"black\", align=\"mid\")\n",
    "plt.xlabel(\"Number of lanes per image\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Lane Count Distribution Across All Datasets\")\n",
    "plt.xticks(np.arange(min_value, max_value + 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89623199",
   "metadata": {},
   "source": [
    "This gives us an understanding of how many lanes each image has, and we see that predominantly it is one lane per image and this reflects well on our goal as we are trying to develop a model for lane keeping assistance, and thus focusing on the lane that we are in is more important that picking up lanes next to the current one or in opposing directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d368c",
   "metadata": {},
   "source": [
    "## **Step 4: Implement a PyTorch Dataset Class**\n",
    "Follow [this tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) to prepare data for deep learning by creating a PyTorch Dataset class that:\n",
    "- Loads data from a CSV or another source.\n",
    "- Applies preprocessing (e.g., normalization, missing value handling).\n",
    "- Returns samples in a PyTorch-compatible format.\n",
    "\n",
    "### **Example PyTorch Dataset Implementation**\n",
    "Modify this template for your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "class LaneDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Path to the directory with images.\n",
    "            label_dir (str): Path to the directory with lane line annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        image_filenames = {f.split('.')[0]: f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))}\n",
    "        label_filenames = {f.split('.')[0]: f for f in os.listdir(label_dir) if f.endswith('.txt')}\n",
    "        self.filenames = sorted(image_filenames.keys() & label_filenames.keys())\n",
    "\n",
    "        self.image_files = [image_filenames[f] for f in self.filenames]\n",
    "        self.label_files = [label_filenames[f] for f in self.filenames]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label_path = os.path.join(self.label_dir, self.label_files[idx])\n",
    "        lanes = []\n",
    "        with open(label_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = list(map(float, line.strip().split()))\n",
    "                if len(parts) < 3:\n",
    "                    continue\n",
    "                class_id = int(parts[0])\n",
    "                points = parts[1:]\n",
    "                lanes.append((class_id, torch.tensor(points).view(-1, 2)))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, lanes\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((360, 640)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n",
    "])\n",
    "\n",
    "# German roads dataset\n",
    "german_dataset = LaneDataset(\n",
    "    image_dir=\"../Maanasa/train/images\",\n",
    "    label_dir=\"../Maanasa/train/labels\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Indian roads dataset\n",
    "indian_dataset = LaneDataset(\n",
    "    image_dir=\"../Autonomous/train/images\",\n",
    "    label_dir=\"../Autonomous/train/labels\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# US roads dataset\n",
    "us_dataset = LaneDataset(\n",
    "    image_dir=\"../ComputerVisionGroup/train/images\",\n",
    "    label_dir=\"../ComputerVisionGroup/train/labels\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(len(german_dataset))\n",
    "print(len(indian_dataset))\n",
    "print(len(us_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcf39c",
   "metadata": {},
   "source": [
    "## **Final Submission**\n",
    "Upload your submission for Milestone 1 to Canvas. \n",
    "Submit this notebook with:\n",
    "\n",
    "âœ… A **clear problem statement**.  \n",
    "âœ… Three **documented datasets** with justification.  \n",
    "âœ… **Exploratory analysis** with summary statistics & visualizations.  \n",
    "âœ… A **PyTorch Dataset class** for preparing data.  \n",
    "\n",
    "ðŸ“Œ Use the provided example to guide your work. Happy Deep Learning! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4592b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Paths to datasets ---\n",
    "datasets = {\n",
    "    \"Dataset1\": {\"image_dir\": \"../Maanasa/train/images\", \"label_dir\": \"../Maanasa/train/labels\"},\n",
    "    \"Dataset2\": {\"image_dir\": \"../Autonomous/train/images\", \"label_dir\": \"../Autonomous/train/labels\"},\n",
    "    \"Dataset3\": {\"image_dir\": \"../ComputerVisionGroup/train/images\", \"label_dir\": \"../ComputerVisionGroup/train/labels\"},\n",
    "}\n",
    "\n",
    "# --- Pad or truncate lane points to exactly 20 ---\n",
    "def fix_lane_points(points, num_points=20):\n",
    "    points = np.array(points)  # Convert list to numpy array\n",
    "    if points.shape[0] < num_points:  # If fewer than 20, pad with last point\n",
    "        pad_amount = num_points - points.shape[0]\n",
    "        pad_values = np.tile(points[-1], (pad_amount, 1))  # Repeat last point\n",
    "        points = np.vstack((points, pad_values))\n",
    "    elif points.shape[0] > num_points:  # If more than 20, truncate\n",
    "        points = points[:num_points]\n",
    "    return points\n",
    "\n",
    "# --- Pad lane tensors ---\n",
    "def pad_lanes(lanes, max_lanes=5, num_points=20):\n",
    "    padded = np.zeros((max_lanes, num_points, 2))  # Default: all zeros\n",
    "    num_lanes = min(len(lanes), max_lanes)\n",
    "    \n",
    "    for i in range(num_lanes):\n",
    "        padded[i] = fix_lane_points(lanes[i], num_points)  # Fix lane size\n",
    "\n",
    "    return torch.tensor(padded, dtype=torch.float32)\n",
    "\n",
    "# --- Dataset Class ---\n",
    "class LaneDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, max_lanes=5, num_points=20):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.max_lanes = max_lanes\n",
    "        self.num_points = num_points\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load lane labels\n",
    "        label_file = self.image_files[idx].replace('.jpg', '.txt').replace('.png', '.txt')\n",
    "        label_path = os.path.join(self.label_dir, label_file)\n",
    "        lanes = []\n",
    "\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = list(map(float, line.strip().split()))\n",
    "                    if len(parts) < 3:\n",
    "                        continue  # Skip invalid lines\n",
    "                    points = np.array(parts[1:]).reshape(-1, 2)  # Convert to (N, 2)\n",
    "                    lanes.append(points)\n",
    "\n",
    "        # Convert to tensor and pad\n",
    "        lanes = pad_lanes(lanes, self.max_lanes, self.num_points)\n",
    "        return image, lanes\n",
    "\n",
    "# --- Custom Collate Function ---\n",
    "def custom_collate(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)  \n",
    "    return images, torch.stack(labels)  \n",
    "\n",
    "# --- DataLoader ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "datasets_list = []\n",
    "for dataset in datasets.values():\n",
    "    datasets_list.append(LaneDataset(dataset[\"image_dir\"], dataset[\"label_dir\"], transform=transform))\n",
    "\n",
    "full_dataset = torch.utils.data.ConcatDataset(datasets_list)\n",
    "\n",
    "# --- Create Train Loader ---\n",
    "train_loader = DataLoader(full_dataset, batch_size=8, collate_fn=custom_collate, shuffle=True)\n",
    "\n",
    "# --- Verify Batch ---\n",
    "batch_images, batch_labels = next(iter(train_loader))\n",
    "print(f\"Batch Image Shape: {batch_images.shape}\")  # [8, 3, 640, 640]\n",
    "print(f\"Example Label Shape: {batch_labels.shape}\")  # [8, 5, 20, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907554d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LightweightLaneDetection(nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(LightweightLaneDetection, self).__init__()\n",
    "        self.debug = debug\n",
    "        \n",
    "        # Input: (batch_size, 3, 640, 640)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Layer 1: 640 -> 320\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Layer 2: 320 -> 160\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Layer 3: 160 -> 80\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Layer 4: 80 -> 40\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            # Layer 5: 40 -> 20\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        # Final conv output: (batch_size, 32, 20, 20)\n",
    "        self.conv_output_size = 32 * 20 * 20  # = 12800\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.conv_output_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 5 * 20 * 2)  # 5 lanes, 20 points, 2 coordinates each\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.debug:\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.conv_layers(x)\n",
    "        if self.debug:\n",
    "            print(f\"After conv shape: {x.shape}\")\n",
    "            print(f\"Flattened size should be: {x.shape[0]} x {x.shape[1] * x.shape[2] * x.shape[3]}\")\n",
    "        \n",
    "        x = self.fc_layers(x)\n",
    "        if self.debug:\n",
    "            print(f\"Output shape before view: {x.shape}\")\n",
    "        \n",
    "        return x.view(-1, 5, 20, 2)\n",
    "\n",
    "def initialize_training(learning_rate=0.001, debug=True):\n",
    "    model = LightweightLaneDetection(debug=debug)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        print(model)\n",
    "        print(\"\\nModel Parameters:\")\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using MPS device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU device\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return model, criterion, optimizer, device\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        if batch_idx == 0 and model.debug:\n",
    "            print(f\"\\nBatch shapes:\")\n",
    "            print(f\"Images: {images.shape}\")\n",
    "            print(f\"Labels: {labels.shape}\")\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        if batch_idx == 0 and model.debug:\n",
    "            print(f\"Outputs: {outputs.shape}\")\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Memory cleanup\n",
    "        images = images.cpu()\n",
    "        labels = labels.cpu()\n",
    "        outputs = outputs.cpu()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# No need to resize to 256x256 since your images are already 640x640\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Small batch size for M3 MacBook\n",
    "train_loader = DataLoader(full_dataset, batch_size=4, collate_fn=custom_collate, shuffle=True)\n",
    "\n",
    "# Initialize with debug\n",
    "model, criterion, optimizer, device = initialize_training(debug=True)\n",
    "\n",
    "# Train\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    avg_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f'Average Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcfc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the image directory and model weights\n",
    "image_directory = '../Autonomous/valid/images'  # Replace with your image directory\n",
    "model_path = 'lane_detection_model.pth'  # Replace with your model weights file path\n",
    "\n",
    "# Load the model (your model architecture)\n",
    "class LightweightLaneDetection(torch.nn.Module):\n",
    "    def __init__(self, debug=False):\n",
    "        super(LightweightLaneDetection, self).__init__()\n",
    "        self.debug = debug\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.MaxPool2d(2),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv_output_size = 32 * 20 * 20  # 12800\n",
    "        \n",
    "        self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(self.conv_output_size, 1024),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(1024, 5 * 20 * 2)  # 5 lanes, 20 points, 2 coordinates each\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.view(-1, 5, 20, 2)\n",
    "\n",
    "# Load the pre-trained model weights\n",
    "model = LightweightLaneDetection(debug=False)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Transform to resize and normalize the image to the model's expected input size\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((640, 640)),  # Resize to 640x640\n",
    "    transforms.ToTensor(),  # Converts to tensor and normalizes to [0, 1]\n",
    "])\n",
    "\n",
    "# Get all image files from the directory\n",
    "image_files = [f for f in os.listdir(image_directory) if f.endswith('.png') or f.endswith('.jpg')]\n",
    "\n",
    "# Process each image\n",
    "for image_file in image_files:\n",
    "    img = plt.imread(os.path.join(image_directory, image_file))  # Load image\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    # Preprocess the image\n",
    "    img_resized = np.array(img)  # Convert to numpy array\n",
    "    img_tensor = transform(img_resized)  # Apply transform\n",
    "    img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension (1, 3, 640, 640)\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        predicted_output = model(img_tensor)  # Get the prediction\n",
    "\n",
    "    # Reshape the output: (5, 20, 2) -> 5 lanes, each with 20 points (x, y) coordinates\n",
    "    predicted_output = predicted_output.squeeze().cpu().numpy()  # Shape: (5, 20, 2)\n",
    "\n",
    "    # Get image dimensions (height, width)\n",
    "    img_height, img_width, _ = img.shape\n",
    "\n",
    "    # Extract lane coordinates from predicted output and draw them\n",
    "    for lane_idx in range(predicted_output.shape[0]):\n",
    "        lane_coords = predicted_output[lane_idx]  # Shape: (20, 2) for 20 points (x, y)\n",
    "\n",
    "        # Convert normalized coordinates (0 to 1) into pixel values\n",
    "        points = [(int(x * img_width), int(y * img_height)) for x, y in lane_coords]\n",
    "\n",
    "        # Draw lane lines on the image\n",
    "        for i in range(len(points) - 1):\n",
    "            img_resized = cv2.line(img_resized, points[i], points[i + 1], (0, 255, 0), 3)  # Green line\n",
    "\n",
    "    # Show the annotated image with matplotlib (without saving)\n",
    "    plt.imshow(img_resized)\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
